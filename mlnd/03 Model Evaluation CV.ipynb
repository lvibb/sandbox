{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Intensive - Machine Learning Nanodegree\n",
    "# Lesson 03: Classification Metrics and CrossValidation\n",
    "\n",
    "## Objectives\n",
    "  - Experiment with building predictive models using the [ `sklearn` library](http://scikit-learn.org/stable/).\n",
    "   - Learn about evaluation metrics for classification [ `sklearn.metrics` ]\n",
    "   - Confusion matrix, precision and recall \n",
    "   - Review Model fitting, prediction and scoring\n",
    "   - Cross Validation and Hyper-parameter Tuning\n",
    "  \n",
    "## Prerequisites\n",
    "  - You should have the following python packages installed:\n",
    "    - [matplotlib](http://matplotlib.org/index.html) (not a pre-reqisite for this part)\n",
    "    - [numpy](http://www.scipy.org/scipylib/download.html)\n",
    "    - [pandas](http://pandas.pydata.org/getpandas.html)\n",
    "    - [sklearn](http://scikit-learn.org/stable/install.html)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "As usual, we start by importing some useful libraries and modules. Don't worry if you get a warning message when importing `matplotlib` -- it just needs to build the font cache, and the warning is just to alert you that this may take a while the first time the cell is run.\n",
    "\n",
    "**Run** the cell below to import useful libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported matplotlib.pyplot! (Version 2.1.2)\n",
      "Successfully imported numpy! (Version 1.14.1)\n",
      "Successfully imported pandas! (Version 0.20.3)\n",
      "Successfully imported display from IPython.display!\n",
      "Successfully imported sklearn! (Version 0.19.1)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "try:\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('ggplot')\n",
    "    print(\"Successfully imported matplotlib.pyplot! (Version {})\".format(matplotlib.__version__))\n",
    "except ImportError:\n",
    "    print(\"Could not import matplotlib.pyplot!\")\n",
    "    \n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    print(\"Could not import numpy!\")\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    print(\"Successfully imported display from IPython.display!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display\")\n",
    "    \n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"Successfully imported sklearn! (Version {})\".format(sklearn.__version__))\n",
    "    skversion = int(sklearn.__version__[2:4])\n",
    "except ImportError:\n",
    "    print(\"Could not import sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 | Classification Metrics\n",
    "\n",
    "#### Loading the data from the Titanic project\n",
    "\n",
    "#### *You will need to specify the proper path to the dataset file or copy the file into the same directory as this notebook*\n",
    "\n",
    "\n",
    "**Run** the cell below (**click** on the cell to highlight it, then press **shift + enter** or **shift + return** to run it) to read the preprocessed training and testing data into `pandas` `DataFrame` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic data sets loaded!\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./projects/titanic_survival_exploration/titanic_data.csv\")\n",
    "\n",
    "print(\"Titanic data sets loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are pretty familiar with the titanic dataset, I can just run through some quick pre-procesing steps that should be familiar to you. You can split the cell below after any line use appropriate print functions to see what each step s doing if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can run this cell ONCE ONLY as the get_dummies method will replace the \"Sex\" and \"Embarked\" columns\n",
    "train_df.loc[train_df.Age.isnull(),'Age'] = train_df.Age.mean()\n",
    "train_df = pd.get_dummies(train_df, columns=[\"Sex\", \"Embarked\"], prefix=[\"sex\", \"port\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data (891 instances) were split into training (623 instances) and testing (268 instances) data sets\n"
     ]
    }
   ],
   "source": [
    "# Starting with scikit-learn version 0.18, the model_selection module replaces the cross_validation module,\n",
    "# so we should import train_test_split from the appropriate module depending on the version number.\n",
    "if skversion >= 18:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "else:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# A list of the desired feature names for model building\n",
    "# We are skipping Passenger ID as that is too specific and is more a row label than a feature\n",
    "# Same arguments lead us to exclude `Cabin` and `Ticket`\n",
    "desired_features = ['Pclass', 'sex_female', 'sex_male', 'Age','SibSp','Parch', 'Fare', 'port_C', 'port_S', 'port_Q']\n",
    "\n",
    "# X is our pandas DataFrame object with the features from which we will predict the 'Survived' feature.\n",
    "X = pd.DataFrame(train_df[desired_features])\n",
    "\n",
    "# y is our pandas Series object with the 'Survived' feature to be predicted.\n",
    "y = pd.Series(train_df['Survived'])\n",
    "\n",
    "# Split the data into training and validation (test) data sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                    random_state=3)\n",
    "\n",
    "# Verify that the data sets were split 80% training and 20% testing\n",
    "print(\"The original data ({} instances) were split into training ({} instances) and testing ({} instances) data sets\".\\\n",
    "     format(len(X),len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making some basic predictions\n",
    "\n",
    "Recall that the key feature we will attempt to predict is the `'Survived'` feature, which is equal to 0 or 1 for a passenger who died or survived, respectively, from the Titanic sinking. We have separated it out as the `y_train` and `y_test` variables.\n",
    "\n",
    "We'll try several sets of predictions and calculate some metrics to evaluate our 'model'\n",
    "\n",
    "A commonly used metric for classification is accuracy_score, which is simply the proportion of correct predictions. If a model predicts m classes of n possible correctly, then the accuracy score will be m / n.\n",
    "\n",
    "The accuracy_score simply ignores wrong predictions. In some situations, we may care about making wrong predictions; the F1 score is a measure that combines both correct and incorrect predictions\n",
    "\n",
    "Let's import the metrics from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, fbeta_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Formulas for Classification Scores\n",
    "\n",
    "$$ Precision (p) = {True\\space Positive \\over (All\\space Positive)} $$\n",
    "\n",
    "$$ Precision (p) = {True\\space Positive \\over (True\\space Positive + False\\space Positive)} $$\n",
    "\n",
    "$$ Recall (r) = {True\\space Positive \\over (True\\space Positive + False\\space Negative)} $$\n",
    "\n",
    "$$ Accuracy = {True\\space Positive + True \\space Negative \\over (All\\space Samples)} $$\n",
    "\n",
    "\n",
    "\n",
    "$$ F_1\\space score = {2  p  r \\over (p + r)} $$\n",
    "\n",
    "$$ F_{\\beta}\\space score = (1 + \\beta^2) {p \\dot r \\over (\\beta^2p + r)} $$\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix for binary classes is often used to provide a compact summary of correct and incorrect predictions. The ground truth is listed down the side and the predicted values are listed along the top. The actual values in each cell of the corresponding grid is the count of cases for which both the ground truth and the predicted value hold.\n",
    "\n",
    " Total Pop | Predicted cond is negative | Predicted cond is positive \n",
    " -------- | -------- | -------- \n",
    " Ground cond is False |  True Negative (TN) | False Positive (FP) \n",
    " Ground cond is True | False Negative (FN) | True Positive (TP)\n",
    " \n",
    " Some commonly used terms:\n",
    " - Precision = True Positive / All Positive = TP / ( TP + FP )\n",
    " - Recall = True Positive / All True = TP / (TP + FN)\n",
    " - Accuracy = True Positive + True Negative)/Total Population = (TP+TN)/(TP+TN+FP+FN)\n",
    " \n",
    " There are many other terms and ratios described in the lecture videos \n",
    " (also this Wiki article https://en.wikipedia.org/wiki/Precision_and_recall )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some simple predictions and calculate some of these by hand (a couple of times) using our training set.\n",
    "We'll start with predicting no one survived. So the predictions array will be as long as y_train and filled with 0.\n",
    "\n",
    "** Exercise ** Implement the four functions below. Use them to calculate the counts of true positive, false negative, false positive, and true negatives. Finally, calculate the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 0.6180\n"
     ]
    }
   ],
   "source": [
    "no_survivors = np.zeros(len(y_train))\n",
    "\n",
    "def true_positives(yt, ypred): \n",
    "    x = pd.DataFrame({'yt': yt, 'pred':ypred})\n",
    "    return x[(x.yt==1) & (x.pred==1)].shape[0]\n",
    "\n",
    "def false_positives(yt, ypred): \n",
    "    x = pd.DataFrame({'yt': yt, 'pred':ypred})\n",
    "    return  x[(x.yt==0) & (x.pred==1)].shape[0]\n",
    "\n",
    "def true_negatives(yt, ypred): \n",
    "    x = pd.DataFrame({'yt': yt, 'pred':ypred})\n",
    "    return x[(x.yt==0) & (x.pred==0)].shape[0]\n",
    "\n",
    "def false_negatives(yt, ypred): \n",
    "    x = pd.DataFrame({'yt': yt, 'pred':ypred})\n",
    "    return x[(x.yt==1) & (x.pred==0)].shape[0]\n",
    "\n",
    "TP = true_positives(y_train, no_survivors)\n",
    "FN = false_negatives(y_train, no_survivors)\n",
    "FP = false_positives(y_train, no_survivors)\n",
    "TN = true_negatives(y_train, no_survivors)\n",
    "\n",
    "accuracy = (TP + TN) / float(TP+FN+FP+TN)\n",
    "print(\"Accuracy score is {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the appropriate functions from `sklearn` (already imported) to compare with your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.6180\n"
     ]
    }
   ],
   "source": [
    "#print(\"Precision score: {:.3f}\".format(precision_score(y_train, no_survivors)))\n",
    "print(\"Accuracy score: {:.4f}\".format(accuracy_score(y_train, no_survivors)))\n",
    "#print y_train\n",
    "#print X_train.sex_female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy scores should match! We couldn't calculate the precision score because we need _some_ positive predictions to get a meaningful value. \n",
    "\n",
    "**Exercise:** Once you have that working, let's calculate the precision (by hand) where our prediction is that all female passengers survived and all males did not. Remember to use X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score is 0.7560\n",
      "Precision score: 0.7560\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "all_females = X_train.sex_female\n",
    "\n",
    "TP = true_positives(y_train, all_females)\n",
    "FN = false_negatives(y_train, all_females)\n",
    "FP = false_positives(y_train, all_females)\n",
    "TN = true_negatives(y_train, all_females)\n",
    "\n",
    "#TODO - implement the expression for precision\n",
    "precision = (TP) / float(TP+FP) # Remember to make the denominator a float \n",
    "accuracy = (TP + TN) / float(TP+FN+FP+TN)\n",
    "\n",
    "print(\"Precision score is {:.4f}\".format(precision))\n",
    "print(\"Precision score: {:.4f}\".format(precision_score(y_train, all_females)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few other ways we can evaluate errors in a classification problem. The confusion matrix should be the same as the first two lines we printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 51\n",
      "80 158\n",
      "Accuracy score: 0.7897\n",
      "f1 score: 0.7069\n",
      "[[334  51]\n",
      " [ 80 158]]\n"
     ]
    }
   ],
   "source": [
    "print TN, FP\n",
    "print FN, TP\n",
    "print(\"Accuracy score: {:.4f}\".format(accuracy_score(y_train, all_females)))\n",
    "print(\"f1 score: {:.4f}\".format(f1_score(y_train, all_females)))\n",
    "print confusion_matrix(y_true=y_train, y_pred=all_females)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the precision we calculate for the `all_females` is lower than the accuracy score. This often happens when the classes are _imbalanced_, i.e., the number of cases of one class is much larger than the number of the other class.\n",
    "\n",
    "In some cases, e.g, detecting fraudulent credit card tansactions, we may be much more interested in correctly predicting the rare cases than in correctly predicting the common ones. We can look at the $f_1 score$ (the harmonic mean of the precision or recall), the $f_{\\beta} score$. We can also flip our class labels. The `classification_report` provides a quick way to look at \n",
    "precision, recall, f1-score and support (number of samples of that class) to get a better feel for how well we did with the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.87      0.84       385\n",
      "          1       0.76      0.66      0.71       238\n",
      "\n",
      "avg / total       0.79      0.79      0.79       623\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y_train, all_females)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 | Cross-Validation\n",
    "\n",
    "### Build an optimized Decision Tree Classifier\n",
    "\n",
    "For supervised learning problems, the model building `sklearn` workflow is pretty similar, regardless of the type of classifier you'd like to build. You should be getting used to this pattern:\n",
    "  1. **Create** a classifier object.\n",
    "  2. **Train** the classifier on the training data set.\n",
    "  3. **Predict** with the classifier on the validation (test) data set.\n",
    "  4. **Assess** the quality of the predictions of the classifier\n",
    "\n",
    "Last week we built a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and looked at a couple of models using `max_depth` as a (hyper)parameter. We used that as an illustration of model complexity and a segue for introducing complexity curves. In your project, you will need to use GridSearchCV. Our objective here is to work through some of those details here.\n",
    "\n",
    "I have already completed the imports for you\n",
    "**Exercise** Complete the 4 TODO sections in the code cell below to **create** a Decision Tree Classifier, **train** it on the training data using k-fold cross-validation to find the optimal DecisionTree moel for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.0021596 , 0.00309658, 0.00391936, 0.00381239, 0.00237131,\n",
       "        0.00236162, 0.00247876, 0.00238427, 0.00232657, 0.00246692]),\n",
       " 'mean_score_time': array([0.00059438, 0.00113646, 0.00147629, 0.00089765, 0.0005947 ,\n",
       "        0.00055965, 0.00055838, 0.00051173, 0.0005544 , 0.00052571]),\n",
       " 'mean_test_score': array([0.78972713, 0.7752809 , 0.80577849, 0.80898876, 0.80417335,\n",
       "        0.78812199, 0.78972713, 0.79614767, 0.79775281, 0.80256822]),\n",
       " 'mean_train_score': array([0.78972346, 0.80257107, 0.83228044, 0.85554472, 0.87080391,\n",
       "        0.89808621, 0.91493636, 0.93098136, 0.94623281, 0.95586368]),\n",
       " 'param_max_depth': masked_array(data=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 1},\n",
       "  {'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 7},\n",
       "  {'max_depth': 8},\n",
       "  {'max_depth': 9},\n",
       "  {'max_depth': 10}],\n",
       " 'rank_test_score': array([ 7, 10,  2,  1,  3,  9,  7,  6,  5,  4], dtype=int32),\n",
       " 'split0_test_score': array([0.79425837, 0.79425837, 0.79425837, 0.784689  , 0.77511962,\n",
       "        0.76076555, 0.74641148, 0.76555024, 0.77033493, 0.77033493]),\n",
       " 'split0_train_score': array([0.78743961, 0.80434783, 0.84299517, 0.85990338, 0.88164251,\n",
       "        0.9057971 , 0.92028986, 0.93236715, 0.94927536, 0.9589372 ]),\n",
       " 'split1_test_score': array([0.75362319, 0.75362319, 0.79710145, 0.80193237, 0.80676329,\n",
       "        0.80193237, 0.8115942 , 0.81642512, 0.82608696, 0.82608696]),\n",
       " 'split1_train_score': array([0.80769231, 0.80769231, 0.82932692, 0.85817308, 0.87019231,\n",
       "        0.89182692, 0.91586538, 0.92788462, 0.94471154, 0.95192308]),\n",
       " 'split2_test_score': array([0.82125604, 0.77777778, 0.82608696, 0.84057971, 0.83091787,\n",
       "        0.80193237, 0.8115942 , 0.80676329, 0.79710145, 0.8115942 ]),\n",
       " 'split2_train_score': array([0.77403846, 0.79567308, 0.82451923, 0.84855769, 0.86057692,\n",
       "        0.89663462, 0.90865385, 0.93269231, 0.94471154, 0.95673077]),\n",
       " 'std_fit_time': array([0.00027698, 0.00078392, 0.00071984, 0.00104476, 0.00048115,\n",
       "        0.00018225, 0.00031102, 0.00052476, 0.00036999, 0.00041205]),\n",
       " 'std_score_time': array([8.99195284e-05, 4.32059126e-04, 8.82312592e-04, 1.84179034e-04,\n",
       "        1.13292792e-04, 4.42604612e-05, 6.49307368e-05, 2.89161102e-05,\n",
       "        9.96411560e-05, 3.49823243e-05]),\n",
       " 'std_test_score': array([0.02775401, 0.01669553, 0.01437271, 0.02337154, 0.02287064,\n",
       "        0.01943715, 0.03077639, 0.02209373, 0.0227833 , 0.02365177]),\n",
       " 'std_train_score': array([0.01383371, 0.00506512, 0.00782656, 0.00499082, 0.00861086,\n",
       "        0.00579493, 0.00479559, 0.00219375, 0.00215141, 0.00292841])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# TODO: Pick a value between 3 and 10 for the number of folds for k-fold cross-validation\n",
    "nfolds = 3\n",
    "\n",
    "# TODO: Create a decision tree regressor object\n",
    "clf = DecisionTreeClassifier(random_state=3)\n",
    "\n",
    "# TODO: Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n",
    "params = {'max_depth': np.arange(1, 11)}\n",
    "\n",
    "# TODO: Create the grid search object\n",
    "grid = GridSearchCV(estimator=clf, param_grid=params, cv=nfolds, return_train_score=True)\n",
    "\n",
    "# TODO: Fit the grid search object to the data to compute the optimal model\n",
    "grid.fit(X_train, y_train)\n",
    "#Prediction - for test set\n",
    "grid_best_fit = grid.best_estimator_.predict(X_test) #best estimator takes the rank_test_score=1, which has the best hyperparameters\n",
    "\n",
    "grid.cv_results_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV returns a lot of intermediate results. You can view them all through `cv_results_`. Within these, `mean_test_score` and `mean_train_score` gives you the data you would need to produce a complexity curve. `rank_test_score` gives the rank (1 is best) for each combination of parameters (we only have 1 here).\n",
    "\n",
    "** Naive Question ** We didn't pass in a test set, how how did we get a test score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "here the test score is based on the test fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question: ** What is the accuracy score and f1-score for the best classifier on the training set? How do these compare to the score on the test set? How would you interpret the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score test set:  0.753731343283582\n",
      "F1-Score test set:  0.6632653061224489\n",
      "Accuracy Score train set:  0.8507223113964687\n",
      "F1-Score test set:  0.7811764705882352\n"
     ]
    }
   ],
   "source": [
    "#print accuracy score\n",
    "print \"Accuracy Score test set: \", accuracy_score(y_test, grid_best_fit)\n",
    "print \"F1-Score test set: \", f1_score(y_test, grid_best_fit)\n",
    "print \"Accuracy Score train set: \", accuracy_score(y_train, grid.best_estimator_.predict(X_train))\n",
    "print \"F1-Score test set: \", f1_score(y_train, grid.best_estimator_.predict(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Cross validation exposed\n",
    "\n",
    "In the cell below, I have exposed how cross-valiation works inside GridSearchCV (and other classes implementing CV in `sklearn`). The KFold.split method is an iterator that returns the list of indices of the training and test sets in the context of CV. We have referred to it as train and validation sets elsewhere. Add print statements to see how the splitting works.\n",
    "\n",
    "The average of `results` would be the reported CV score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8106419546636937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "#Simple K-Fold cross validation. \n",
    "cv = KFold(n_splits=3)\n",
    "results = []\n",
    "\n",
    "# \"Error_function\" can be replaced by the error function of your analysis\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=3)\n",
    "for traincv, testcv in cv.split(X_train):\n",
    "    #print testcv\n",
    "    kscore = model.fit(X_train.iloc[traincv], y_train.iloc[traincv]).score(X_train.iloc[testcv], y_train.iloc[testcv])\n",
    "    results.append(kscore)\n",
    "print np.mean(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  That's all for this notebook !!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
